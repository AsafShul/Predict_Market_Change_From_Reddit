{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPAvcu+l6d4WZLXRZ7/RWPn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uv619Sn80Awj","executionInfo":{"status":"ok","timestamp":1685667683356,"user_tz":-180,"elapsed":20573,"user":{"displayName":"Yuval Arbel","userId":"02876933895760826567"}},"outputId":"5b707a50-4882-4380-b7e1-d57992fb1033"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q transformers\n","!pip install -q wandb\n","!pip install -q evaluate\n","!pip install -q -U accelerate\n","!pip install -q -U pyarrow"]},{"cell_type":"code","source":["!python src/baseline.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iob6dnjK0y_L","executionInfo":{"status":"ok","timestamp":1685672410197,"user_tz":-180,"elapsed":823092,"user":{"displayName":"Yuval Arbel","userId":"02876933895760826567"}},"outputId":"7f369e39-1488-4586-a350-5303c3ebd2b7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Testing day by day:   0% 0/251 [00:00<?, ?it/s]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:   0% 1/251 [00:02<09:54,  2.38s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   1% 2/251 [00:05<10:49,  2.61s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:   1% 3/251 [00:08<11:35,  2.81s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:   2% 4/251 [00:12<13:31,  3.29s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:   2% 5/251 [00:15<14:02,  3.43s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:   2% 6/251 [00:19<13:53,  3.40s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:   3% 7/251 [00:22<14:12,  3.49s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   3% 8/251 [00:27<15:40,  3.87s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:   4% 9/251 [00:31<15:32,  3.85s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   4% 10/251 [00:35<15:35,  3.88s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:   4% 11/251 [00:40<16:59,  4.25s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   5% 12/251 [00:45<17:32,  4.41s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   5% 13/251 [00:49<17:31,  4.42s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:   6% 14/251 [00:57<21:23,  5.42s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:   6% 15/251 [01:02<21:23,  5.44s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:   6% 16/251 [01:09<23:03,  5.89s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:   7% 17/251 [01:15<23:06,  5.93s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:   7% 18/251 [01:22<23:47,  6.13s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:   8% 19/251 [01:27<22:21,  5.78s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:   8% 20/251 [01:28<17:11,  4.46s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:   8% 21/251 [01:30<13:56,  3.64s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:   9% 22/251 [01:32<11:55,  3.12s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:   9% 23/251 [01:36<12:30,  3.29s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  10% 24/251 [01:38<11:57,  3.16s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  10% 25/251 [01:40<10:06,  2.68s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  10% 26/251 [01:42<09:32,  2.54s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  11% 27/251 [01:45<09:12,  2.47s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  11% 28/251 [01:47<09:03,  2.44s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  12% 29/251 [01:49<08:38,  2.34s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  12% 30/251 [01:50<07:35,  2.06s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  12% 31/251 [01:52<07:13,  1.97s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  13% 32/251 [01:54<07:29,  2.05s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  13% 33/251 [01:57<07:39,  2.11s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  14% 34/251 [02:00<08:26,  2.33s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  14% 35/251 [02:01<07:52,  2.19s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  14% 36/251 [02:03<07:47,  2.17s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  15% 37/251 [02:06<08:06,  2.27s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  15% 38/251 [02:08<07:39,  2.16s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  16% 39/251 [02:10<07:08,  2.02s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  16% 40/251 [02:11<06:40,  1.90s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  16% 41/251 [02:13<06:20,  1.81s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  17% 42/251 [02:15<06:19,  1.81s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  17% 43/251 [02:17<06:28,  1.87s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  18% 44/251 [02:19<06:40,  1.94s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  18% 45/251 [02:21<07:11,  2.09s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  18% 46/251 [02:24<07:26,  2.18s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  19% 47/251 [02:25<06:32,  1.92s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  19% 48/251 [02:27<07:07,  2.11s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  20% 49/251 [02:30<07:19,  2.18s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  20% 50/251 [02:32<07:39,  2.29s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  20% 51/251 [02:34<07:29,  2.25s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  21% 52/251 [02:36<06:52,  2.07s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  21% 53/251 [02:38<06:31,  1.98s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  22% 54/251 [02:39<05:42,  1.74s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  22% 55/251 [02:41<05:42,  1.75s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  22% 56/251 [02:44<06:46,  2.09s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  23% 57/251 [02:47<07:38,  2.36s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  23% 58/251 [02:48<06:54,  2.15s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  24% 59/251 [02:50<06:45,  2.11s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  24% 60/251 [02:53<07:05,  2.23s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  24% 61/251 [02:55<06:45,  2.13s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  25% 62/251 [02:57<07:02,  2.23s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  25% 63/251 [03:02<09:00,  2.88s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  25% 64/251 [03:04<08:18,  2.67s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  26% 65/251 [03:06<08:02,  2.60s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  26% 66/251 [03:08<07:33,  2.45s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  27% 67/251 [03:11<07:51,  2.56s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  27% 68/251 [03:14<08:24,  2.76s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  27% 69/251 [03:17<08:11,  2.70s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  28% 70/251 [03:19<07:47,  2.58s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  28% 71/251 [03:22<07:28,  2.49s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  29% 72/251 [03:25<07:50,  2.63s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  29% 73/251 [03:27<07:50,  2.64s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  29% 74/251 [03:30<07:52,  2.67s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  30% 75/251 [03:34<08:47,  3.00s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  30% 76/251 [03:37<09:27,  3.24s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  31% 77/251 [03:42<10:48,  3.73s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  31% 78/251 [03:46<10:25,  3.61s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  31% 79/251 [03:50<10:53,  3.80s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  32% 80/251 [03:56<12:23,  4.35s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  32% 81/251 [04:00<12:22,  4.37s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  33% 82/251 [04:04<11:50,  4.21s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  33% 83/251 [04:07<11:19,  4.04s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  33% 84/251 [04:11<10:26,  3.75s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  34% 85/251 [04:13<09:41,  3.50s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  34% 86/251 [04:17<09:40,  3.52s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  35% 87/251 [04:20<08:56,  3.27s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  35% 88/251 [04:24<09:24,  3.46s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  35% 89/251 [04:27<09:32,  3.54s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  36% 90/251 [04:30<09:00,  3.36s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  36% 91/251 [04:34<09:30,  3.56s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  37% 92/251 [04:38<09:15,  3.50s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  37% 93/251 [04:40<08:12,  3.11s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  37% 94/251 [04:42<07:28,  2.86s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  38% 95/251 [04:45<07:10,  2.76s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  38% 96/251 [04:48<07:33,  2.93s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  39% 97/251 [04:52<08:32,  3.33s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  39% 98/251 [04:55<08:00,  3.14s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  39% 99/251 [04:58<07:41,  3.04s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  40% 100/251 [05:00<07:14,  2.88s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  40% 101/251 [05:03<06:46,  2.71s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  41% 102/251 [05:05<06:44,  2.72s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  41% 103/251 [05:07<06:13,  2.52s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  41% 104/251 [05:09<05:14,  2.14s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  42% 105/251 [05:10<04:55,  2.03s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  42% 106/251 [05:12<04:49,  2.00s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  43% 107/251 [05:14<04:37,  1.93s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  43% 108/251 [05:16<04:47,  2.01s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  43% 109/251 [05:19<05:07,  2.16s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  44% 110/251 [05:21<05:25,  2.31s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  44% 111/251 [05:25<06:08,  2.63s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  45% 112/251 [05:28<06:28,  2.79s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  45% 113/251 [05:32<07:04,  3.07s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  45% 114/251 [05:36<07:58,  3.49s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  46% 115/251 [05:40<08:06,  3.58s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  46% 116/251 [05:44<08:04,  3.59s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  47% 117/251 [05:46<07:30,  3.36s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  47% 118/251 [05:50<07:32,  3.40s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  47% 119/251 [05:53<07:09,  3.25s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  48% 120/251 [05:55<06:40,  3.05s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  48% 121/251 [05:57<05:47,  2.68s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  49% 122/251 [06:00<05:36,  2.61s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  49% 123/251 [06:03<06:03,  2.84s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  49% 124/251 [06:06<05:55,  2.80s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  50% 125/251 [06:08<05:40,  2.71s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  50% 126/251 [06:10<05:17,  2.54s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  51% 127/251 [06:13<05:04,  2.45s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  51% 128/251 [06:16<05:38,  2.75s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  51% 129/251 [06:19<05:33,  2.73s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  52% 130/251 [06:21<04:59,  2.47s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  52% 131/251 [06:24<05:15,  2.63s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  53% 132/251 [06:27<05:40,  2.86s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  53% 133/251 [06:31<06:14,  3.17s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  53% 134/251 [06:34<06:00,  3.08s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  54% 135/251 [06:37<05:56,  3.07s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  54% 136/251 [06:40<05:42,  2.98s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  55% 137/251 [06:43<05:55,  3.12s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  55% 138/251 [06:46<06:01,  3.20s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  55% 139/251 [06:49<05:41,  3.05s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  56% 140/251 [06:51<05:08,  2.78s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  56% 141/251 [06:54<05:05,  2.78s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  57% 142/251 [06:59<06:05,  3.35s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  57% 143/251 [07:03<06:24,  3.56s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  57% 144/251 [07:07<06:37,  3.71s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  58% 145/251 [07:09<05:49,  3.29s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  58% 146/251 [07:13<06:06,  3.49s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  59% 147/251 [07:16<05:58,  3.45s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  59% 148/251 [07:20<05:50,  3.40s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  59% 149/251 [07:25<06:30,  3.83s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  60% 150/251 [07:30<07:05,  4.22s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  60% 151/251 [07:35<07:20,  4.40s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  61% 152/251 [07:39<07:30,  4.56s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  61% 153/251 [07:44<07:23,  4.53s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  61% 154/251 [07:48<06:53,  4.26s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  62% 155/251 [07:53<07:36,  4.76s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  62% 156/251 [08:03<09:41,  6.12s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  63% 157/251 [08:15<12:38,  8.07s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  63% 158/251 [08:30<15:19,  9.88s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  63% 159/251 [08:50<20:09, 13.15s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  64% 160/251 [08:55<16:01, 10.57s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  64% 161/251 [08:59<12:55,  8.61s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  65% 162/251 [09:05<11:36,  7.83s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  65% 163/251 [09:10<10:18,  7.02s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  65% 164/251 [09:15<09:09,  6.32s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  66% 165/251 [09:20<08:38,  6.03s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  66% 166/251 [09:25<08:07,  5.74s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  67% 167/251 [09:31<08:09,  5.82s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  67% 168/251 [09:37<07:52,  5.70s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  67% 169/251 [09:40<07:00,  5.13s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  68% 170/251 [09:43<05:58,  4.42s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  68% 171/251 [09:47<05:51,  4.40s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  69% 172/251 [09:50<04:53,  3.71s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  69% 173/251 [09:52<04:22,  3.37s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  69% 174/251 [09:54<03:47,  2.96s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  70% 175/251 [09:57<03:42,  2.93s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  70% 176/251 [10:01<04:02,  3.23s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  71% 177/251 [10:04<03:51,  3.12s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  71% 178/251 [10:07<03:40,  3.02s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  71% 179/251 [10:09<03:20,  2.78s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  72% 180/251 [10:11<03:03,  2.58s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  72% 181/251 [10:15<03:25,  2.94s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  73% 182/251 [10:18<03:31,  3.07s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  73% 183/251 [10:22<03:36,  3.18s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  73% 184/251 [10:25<03:32,  3.17s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  74% 185/251 [10:29<03:46,  3.43s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  74% 186/251 [10:32<03:48,  3.52s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  75% 187/251 [10:36<03:54,  3.67s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  75% 188/251 [10:41<03:59,  3.81s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  75% 189/251 [10:45<04:07,  3.99s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  76% 190/251 [10:49<04:06,  4.05s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  76% 191/251 [10:53<03:55,  3.92s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  76% 192/251 [10:57<04:04,  4.14s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  77% 193/251 [11:01<03:52,  4.00s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  77% 194/251 [11:04<03:32,  3.73s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  78% 195/251 [11:08<03:35,  3.84s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  78% 196/251 [11:12<03:35,  3.91s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  78% 197/251 [11:17<03:38,  4.05s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  79% 198/251 [11:20<03:14,  3.68s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  79% 199/251 [11:22<02:58,  3.43s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  80% 200/251 [11:26<02:53,  3.41s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  80% 201/251 [11:29<02:43,  3.28s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  80% 202/251 [11:32<02:37,  3.21s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  81% 203/251 [11:35<02:31,  3.15s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  81% 204/251 [11:38<02:27,  3.15s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  82% 205/251 [11:41<02:28,  3.22s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  82% 206/251 [11:45<02:25,  3.24s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  82% 207/251 [11:49<02:36,  3.55s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  83% 208/251 [11:54<02:52,  4.02s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  83% 209/251 [11:57<02:30,  3.59s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  84% 210/251 [12:00<02:21,  3.45s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  84% 211/251 [12:03<02:17,  3.44s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  84% 212/251 [12:07<02:23,  3.67s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  85% 213/251 [12:11<02:13,  3.52s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  85% 214/251 [12:13<02:00,  3.25s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  86% 215/251 [12:15<01:46,  2.96s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  86% 216/251 [12:19<01:45,  3.00s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  86% 217/251 [12:23<01:57,  3.45s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  87% 218/251 [12:26<01:49,  3.33s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  87% 219/251 [12:27<01:27,  2.75s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  88% 220/251 [12:29<01:16,  2.46s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  88% 221/251 [12:31<01:08,  2.27s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  88% 222/251 [12:33<01:05,  2.25s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  89% 223/251 [12:36<01:03,  2.27s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  89% 224/251 [12:37<00:53,  2.00s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  90% 225/251 [12:39<00:51,  2.00s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  90% 226/251 [12:41<00:51,  2.07s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  90% 227/251 [12:42<00:42,  1.78s/it]\n","Pred: 1 ; Label: 1.0\n","Testing day by day:  91% 228/251 [12:44<00:38,  1.66s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  91% 229/251 [12:45<00:35,  1.63s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  92% 230/251 [12:48<00:41,  1.99s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  92% 231/251 [12:51<00:42,  2.12s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  92% 232/251 [12:52<00:37,  1.98s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  93% 233/251 [12:53<00:31,  1.76s/it]\n","Pred: 2 ; Label: 1.0\n","Testing day by day:  93% 234/251 [12:55<00:29,  1.72s/it]\n","Pred: 0 ; Label: 1.0\n","Testing day by day:  94% 235/251 [12:57<00:28,  1.79s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  94% 236/251 [12:59<00:27,  1.83s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  94% 237/251 [13:01<00:26,  1.91s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day:  95% 238/251 [13:03<00:26,  2.02s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  95% 239/251 [13:06<00:27,  2.28s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  96% 240/251 [13:09<00:26,  2.39s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  96% 241/251 [13:11<00:24,  2.42s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  96% 242/251 [13:13<00:20,  2.24s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  97% 243/251 [13:15<00:17,  2.22s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day:  97% 244/251 [13:18<00:16,  2.31s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  98% 245/251 [13:19<00:12,  2.11s/it]\n","Pred: 1 ; Label: 0.0\n","Testing day by day:  98% 246/251 [13:22<00:10,  2.11s/it]\n","Pred: 0 ; Label: 2.0\n","Testing day by day:  98% 247/251 [13:23<00:07,  1.97s/it]\n","Pred: 1 ; Label: 2.0\n","Testing day by day:  99% 248/251 [13:25<00:05,  1.92s/it]\n","Pred: 2 ; Label: 0.0\n","Testing day by day:  99% 249/251 [13:27<00:03,  1.83s/it]\n","Pred: 0 ; Label: 0.0\n","Testing day by day: 100% 250/251 [13:28<00:01,  1.76s/it]\n","Pred: 2 ; Label: 2.0\n","Testing day by day: 100% 251/251 [13:30<00:00,  3.23s/it]\n","F1 Score: 0.26569775842050203\n","Accuracy: 0.26693227091633465\n"]}]},{"cell_type":"code","source":["!python src/finetune.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPqFhpl_7nCs","executionInfo":{"status":"ok","timestamp":1685674447614,"user_tz":-180,"elapsed":195283,"user":{"displayName":"Yuval Arbel","userId":"02876933895760826567"}},"outputId":"ccf9b325-9024-451d-aad1-16a4e3fb6136"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-02 02:51:02.283234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","  0% 0/55155 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  0% 214/55155 [01:37<8:43:49,  1.75it/s]Traceback (most recent call last):\n","  File \"/content/src/finetune.py\", line 148, in <module>\n","    main()\n","  File \"/content/src/finetune.py\", line 138, in main\n","    train_results, test_score = r.train()\n","  File \"/content/src/finetune.py\", line 95, in train\n","    train_result = trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1664, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1940, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2753, in training_step\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/src/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m148\u001b[0m in \u001b[92m<module>\u001b[0m                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m148 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m149 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/src/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m138\u001b[0m in \u001b[92mmain\u001b[0m                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmain\u001b[0m():                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# wandb.login(key=os.environ[\"WANDB_API_KEY\"])\u001b[0m                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   \u001b[0mr = RedditStockPredictionFinetune()                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m138 \u001b[2m│   \u001b[0mtrain_results, test_score = r.train()                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mtrain_results:\u001b[0m\u001b[33m'\u001b[0m)                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(train_results)                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/src/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m95\u001b[0m in \u001b[92mtrain\u001b[0m                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 95 \u001b[2m│   │   \u001b[0mtrain_result = trainer.train()                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   \u001b[0mtrainer.save_model(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mmodels/\u001b[0m\u001b[33m{\u001b[0mMODEL\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   \u001b[0mtest_score = \u001b[96mself\u001b[0m.test()                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# wandb.finish()\u001b[0m                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1664\u001b[0m in      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mtrain\u001b[0m                                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1662 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1664 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1666 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1667 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1940\u001b[0m in      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1938 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inpu \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1940 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1942 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1943 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2753\u001b[0m in      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mtraining_step\u001b[0m                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2750 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in d\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2751 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2752 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2753 \u001b[2m│   │   │   \u001b[0mloss.backward()                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2754 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2755 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2756 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line fu\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ eng\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into th\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/wandb/offline-run-20230602_025226-o1bjwg07\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230602_025226-o1bjwg07/logs\u001b[0m\n","^C\n"]}]}]}